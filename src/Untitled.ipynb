{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain.llms import Cohere, OpenAI, AI21\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import SequentialChain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.embeddings import CohereEmbeddings,OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import DataFrameLoader,CSVLoader\n",
    "import os\n",
    "\n",
    "with open(\"../data/apis.yaml\", \"r\") as file:\n",
    "    yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "open_ai_params = {\n",
    "    \"max_tokens\": 512,\n",
    "    \"openai_api_key\": yaml_data[\"LLMS\"][\"OPENAI_API_KEY\"],\n",
    "}\n",
    "cohere_params = {\n",
    "    \"model\": \"command-xlarge-nightly\",\n",
    "    \"max_tokens\": 2202,\n",
    "    \"cohere_api_key\": yaml_data[\"LLMS\"][\"COHERE_API_KEY\"],\n",
    "    \"temperature\": 0,\n",
    "    \"k\": 0,\n",
    "}\n",
    "ai21_params = {\n",
    "    \"model\": \"j2-jumbo-instruct\",\n",
    "    \"numResults\": 1,\n",
    "    \"temperature\": 0,\n",
    "    \"topP\": 1,\n",
    "    \"ai21_api_key\": yaml_data[\"LLMS\"][\"AI21_API_KEY\"],\n",
    "    \"maxTokens\": 5000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ccbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key = open_ai_params[\"openai_api_key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19cc14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain.llms import Cohere, OpenAI, AI21\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import SequentialChain,AnalyzeDocumentChain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.embeddings import CohereEmbeddings,OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import DataFrameLoader,CSVLoader,SeleniumURLLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import pypdf\n",
    "import os\n",
    "\n",
    "with open(\"../data/apis.yaml\", \"r\") as file:\n",
    "    yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "open_ai_params = {\n",
    "    \"max_tokens\": 512,\n",
    "    \"openai_api_key\": yaml_data[\"LLMS\"][\"OPENAI_API_KEY\"],\n",
    "}\n",
    "cohere_params = {\n",
    "    \"model\": \"command-xlarge-nightly\",\n",
    "    \"max_tokens\": 2202,\n",
    "    \"cohere_api_key\": yaml_data[\"LLMS\"][\"COHERE_API_KEY\"],\n",
    "    \"temperature\": 0,\n",
    "    \"k\": 0,\n",
    "}\n",
    "ai21_params = {\n",
    "    \"model\": \"j2-jumbo-instruct\",\n",
    "    \"numResults\": 1,\n",
    "    \"temperature\": 0,\n",
    "    \"topP\": 1,\n",
    "    \"ai21_api_key\": yaml_data[\"LLMS\"][\"AI21_API_KEY\"],\n",
    "    \"maxTokens\": 25,\n",
    "}\n",
    "\n",
    "\n",
    "class StockLLM:\n",
    "    def __init__(self, ticker):\n",
    "        self.ticker = ticker\n",
    "\n",
    "    def stock_availability(self):\n",
    "        return self.ticker in os.listdir(\"./ticker\")\n",
    "\n",
    "    def sec_analysis_agent(self):\n",
    "        ### Get sec files\n",
    "        file_path = f\"../ticker/{self.ticker}/fa/analysis_sec.txt\"\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                file = f.read()\n",
    "        except:\n",
    "            raise Exception(\"This file is unavailable\")\n",
    "        return file\n",
    "\n",
    "class LLM_analysis:\n",
    "    def __init__(self, ticker, open_ai_params, cohere_params, ai21_params):\n",
    "        ### Requires both Cohere and OpenAI APIs\n",
    "        self.ticker = ticker\n",
    "        self.open_ai_params = open_ai_params\n",
    "        self.cohere_params = cohere_params\n",
    "        self.ai21_params = ai21_params\n",
    "        self.cohere_llm = Cohere(**self.cohere_params)\n",
    "        self.open_ai_llm = OpenAI(**self.open_ai_params)\n",
    "        self.ai21_llm = AI21(**self.ai21_params)\n",
    "        self.stockllm = StockLLM(self.ticker)\n",
    "\n",
    "    def sec_chain_analysis(self):\n",
    "        ### Initally we need a good bullet point summary of the latest sec filings\n",
    "\n",
    "        template = \"\"\"\n",
    "\"This is the sec summary of {stock}.\\n\n",
    "{summary}\\n\"\n",
    "Can you summarize the text into bullet points with numbers in detail. Be as detailed as possible:-\n",
    "\"\"\"\n",
    "        sec_template = PromptTemplate(\n",
    "            template=template, input_variables=[\"stock\", \"summary\"]\n",
    "        )\n",
    "        sec_chain = LLMChain(\n",
    "            llm=self.cohere_llm, prompt=sec_template, output_key=\"sec_summary\"\n",
    "        )\n",
    "        template = \"\"\"You are a financial analyst. Based on the below bullet points, can you further separate them into positive\n",
    "and negative news in bullet points. Please do not leave out any point and go step by step.\n",
    "{sec_summary}\"\"\"\n",
    "        pos_neg_template = PromptTemplate(\n",
    "            template=template, input_variables=[\"sec_summary\"]\n",
    "        )\n",
    "        pos_neg_chain = LLMChain(\n",
    "            llm=self.open_ai_llm, prompt=pos_neg_template, output_key=\"sec_final_output\"\n",
    "        )\n",
    "        overall_chain = SequentialChain(\n",
    "            input_variables=[\"stock\", \"summary\"],\n",
    "            chains=[sec_chain, pos_neg_chain],\n",
    "            # Here we return multiple variables\n",
    "            output_variables=[\"sec_final_output\", \"sec_summary\"],\n",
    "            verbose=True,\n",
    "        )\n",
    "        with get_openai_callback() as cb:\n",
    "            statement = overall_chain(\n",
    "                {\"stock\": self.ticker, \"summary\": self.stockllm.sec_analysis_agent()}\n",
    "            )\n",
    "            cb = {\n",
    "                \"Total Tokens\": cb.total_tokens,\n",
    "                \"Prompt Tokens\": cb.prompt_tokens,\n",
    "                \"Completion Tokens\": cb.completion_tokens,\n",
    "                \"Total Cost (USD)\": cb.total_cost,\n",
    "            }\n",
    "            statement[\"token_summary\"] = cb\n",
    "        return statement\n",
    "\n",
    "    def input_from_user_zero_shot(self, query):\n",
    "        ### Zero shot learning \n",
    "        template = \"\"\"\n",
    "\"\\n\n",
    "{summary}\\n\"\n",
    "Please predict sentiment classification of the above based on above text where sentiment can only be Strongly Positive, Positive, Strongly Negative, Negative, or Neutral. Only output the sentiment class, should be 1 or 2 words.:-\n",
    "\"\"\"\n",
    "        sec_template = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "        return self.open_ai_llm(template.format(summary=query))\n",
    "    \n",
    "    def input_from_user_embedding_shot(self,query):\n",
    "        classifications = ['Strongly Positive','Positive','Neutral','Negative','Strongly Negative']\n",
    "        ### Create embeddings\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key = open_ai_params[\"openai_api_key\"])\n",
    "        ## Create a faiss vector database\n",
    "        faiss_classifications = FAISS.from_texts(classifications,embeddings)\n",
    "        text = faiss_classifications.similarity_search_with_score(query,k = 1)[0][0].page_content\n",
    "        return text\n",
    "        \n",
    "    def input_from_user_sentiment_file(self,file,type_of_file):\n",
    "        if type_of_file not in ['pdf','txt','link','csv']:\n",
    "            raise NotImplementedError(\"This file extension has not been implemented.\")\n",
    "        if type_of_file == 'pdf':\n",
    "            pages = [page.extract_text() for page in pypdf.PdfReader(file).pages]\n",
    "            text = '\\n'.join(pages)\n",
    "\n",
    "        if type_of_file in ['txt','csv']:\n",
    "            with open(file,'r') as f:\n",
    "                text = f.read()\n",
    "        \n",
    "        if type_of_file == 'link':\n",
    "            loader = SeleniumURLLoader(urls=[file])\n",
    "            data = loader.load()\n",
    "            text = data[0].page_content\n",
    "        llm = AI21(temperature=0,ai21_api_key = ai21_params[\"ai21_api_key\"])\n",
    "        summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)\n",
    "        summary = summarize_document_chain.run(text)\n",
    "        final_class = self.input_from_user_embedding_shot(summary)\n",
    "        return final_class\n",
    "    \n",
    "    def query_user(self,file,type_of_file):\n",
    "        if type_of_file not in ['pdf','txt','link','csv']:\n",
    "            raise NotImplementedError(\"This file extension has not been implemented.\")\n",
    "        if type_of_file == 'pdf':\n",
    "            pages = [page.extract_text() for page in pypdf.PdfReader(file).pages]\n",
    "            text = '\\n'.join(pages)\n",
    "\n",
    "        if type_of_file in ['txt','csv']:\n",
    "            with open(file,'r') as f:\n",
    "                text = f.read()\n",
    "        \n",
    "        if type_of_file == 'link':\n",
    "            loader = SeleniumURLLoader(urls=[file])\n",
    "            data = loader.load()\n",
    "            text = data[0].page_content\n",
    "        llm = AI21(temperature=0,ai21_api_key = ai21_params[\"ai21_api_key\"])\n",
    "        summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)\n",
    "        summary = summarize_document_chain.run(text)\n",
    "        final_class = self.input_from_user_embedding_shot(summary)\n",
    "        return final_class\n",
    "    \n",
    "    def qachain(self,vectorstore,query):\n",
    "        documents = vectorstore.as_retriever(search_kwargs={\"k\": 2}).get_relevant_documents(query)\n",
    "        context_full_doc = []\n",
    "        file_names = []\n",
    "        for doc in documents:\n",
    "            page_content = doc.page_content\n",
    "            meta_data = doc.metadata['metadata']\n",
    "            ticker = doc.metadata['ticker']\n",
    "            context_precursor = '''The below contains information about {} and the information is {}'''.format(ticker,meta_data)\n",
    "            context_full= '''{}\n",
    "            {}'''.format(context_precursor,page_content)\n",
    "            context_full_doc.append(context_full)\n",
    "            file_names.append(doc.metadata['file_path'])\n",
    "        context_full_doc.append('''Based on the information above please answer the below question, if the context does not provide the information,just say you don't know \\n Question:''')\n",
    "        context_full_doc.append(query)\n",
    "        context_full_doc = '\\n'.join(context_full_doc)\n",
    "        return context_full_doc,file_names\n",
    "    \n",
    "    def process_file_names(self,file_names):\n",
    "        csv_filter = [file_name for file_name in file_names if '.csv' in file_name]\n",
    "        df = pd.read_csv(csv_filter[0])\n",
    "        df.rename(columns = {'Unnamed: 0':'Description'},inplace = True)\n",
    "        return df\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac064174",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LLM_analysis(None,open_ai_params,cohere_params,ai21_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06eec4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_db = FAISS.load_local('../data/entiredocument',OpenAIEmbeddings(openai_api_key= open_ai_params['openai_api_key']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978527c7",
   "metadata": {},
   "source": [
    "### Checking Yahoo Finance for streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6382b6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PineCone' from 'langchain.vectorstores' (/opt/anaconda3/envs/ts/lib/python3.9/site-packages/langchain/vectorstores/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/maneelreddy/Downloads/Entreprenuer/lstocklopenbbm/src/Untitled.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maneelreddy/Downloads/Entreprenuer/lstocklopenbbm/src/Untitled.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpinecone\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maneelreddy/Downloads/Entreprenuer/lstocklopenbbm/src/Untitled.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m FAISS,PineCone\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PineCone' from 'langchain.vectorstores' (/opt/anaconda3/envs/ts/lib/python3.9/site-packages/langchain/vectorstores/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain.vectorstores import FAISS,Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd47d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
