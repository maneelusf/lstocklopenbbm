{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1e8113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ts/lib/python3.9/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from langchain.llms import Cohere, OpenAI, AI21\n",
    "from langchain.embeddings import CohereEmbeddings,OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS,Pinecone\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab255400",
   "metadata": {},
   "source": [
    "#### MY OWN DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVLoader_v1(BaseLoader):\n",
    "    \"\"\"Loads a CSV file into a list of documents.\n",
    "    Each document represents one row of the CSV file. Every row is converted into a\n",
    "    key/value pair and outputted to a new line in the document's page_content.\n",
    "    The source for each document loaded from csv is set to the value of the\n",
    "    `file_path` argument for all doucments by default.\n",
    "    You can override this by setting the `source_column` argument to the\n",
    "    name of a column in the CSV file.\n",
    "    The source of each document will then be set to the value of the column\n",
    "    with the name specified in `source_column`.\n",
    "    Output Example:\n",
    "        .. code-block:: txt\n",
    "            column1: value1\n",
    "            column2: value2\n",
    "            column3: value3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        source_column= None,\n",
    "        encoding = None,\n",
    "    ):\n",
    "        self.file_path = file_path\n",
    "        self.source_column = source_column\n",
    "        self.encoding = encoding\n",
    "    def load(self):\n",
    "        \"\"\"Load data into document objects.\"\"\"\n",
    "\n",
    "        docs = []\n",
    "        with open(self.file_path,'r') as f:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            ticker = self.file_path.split('/')[2]\n",
    "            meta_data = self.file_path.split('.')[-2].split('/')[-1]\n",
    "            dict1 = {'balance':'Balance Sheet','cash':'Cash Flow','income':'Income Statement',\\\n",
    "                    'ratios':'Key Financial Ratios','est':'Analyst Estimates','fraud':'Fraud Ratios',\n",
    "                    'c_news':'News','s_news':'Sentiment News'}\n",
    "            if meta_data in dict1.keys():\n",
    "                meta_data = dict1[meta_data]\n",
    "            metadata = {\"ticker\": ticker, \"metadata\": meta_data,\"file_path\": self.file_path}\n",
    "            file_content = f.read()\n",
    "        doc = Document(page_content=file_content, metadata=metadata)\n",
    "        return [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apis.yaml\", \"r\") as file:\n",
    "    yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "open_ai_params = {'max_tokens':2000,'openai_api_key' : yaml_data['LLMS']['OPENAI_API_KEY'],'temperature' :0,'model_name':'text-davinci-003'}\n",
    "cohere_params = {\n",
    "    \"model\": \"command-xlarge-nightly\",\n",
    "    \"max_tokens\": 2202,\n",
    "    \"cohere_api_key\": yaml_data[\"LLMS\"][\"COHERE_API_KEY\"],\n",
    "    \"temperature\": 0,\n",
    "    \"k\": 0,\n",
    "}\n",
    "csv_loader = DirectoryLoader('../ticker', glob=\"**/*.csv\", loader_cls=CSVLoader)\n",
    "text_loader = DirectoryLoader('../ticker', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "co = CohereEmbeddings(cohere_api_key=cohere_params[\"cohere_api_key\"])\n",
    "oai = OpenAIEmbeddings(openai_api_key = yaml_data[\"LLMS\"]['OPENAI_API_KEY'])\n",
    "# final_docs = []\n",
    "# for loader in [csv_loader,text_loader]:\n",
    "#     docs = loader.load()\n",
    "#     final_docs.extend(docs)\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# documents = text_splitter.split_documents(final_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa315af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadatagenerator(documents):\n",
    "    for doc in documents:\n",
    "        file_path = doc.metadata['source']\n",
    "        ticker = file_path.split('/')[2]\n",
    "        meta_data = file_path.split('.')[-2].split('/')[-1]\n",
    "        dict1 = {'balance':'Balance Sheet','cash':'Cash Flow','income':'Income Statement',\\\n",
    "                'ratios':'Key Financial Ratios','est':'Analyst Estimates','fraud':'Fraud Ratios',\n",
    "                'c_news':'News','s_news':'Sentiment News'}\n",
    "        if meta_data in dict1.keys():\n",
    "            meta_data = dict1[meta_data]\n",
    "        metadata = {\"ticker\": ticker, \"metadata\": meta_data,\"file_path\": file_path}\n",
    "        doc.metadata = metadata\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = metadatagenerator(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'financial-analysis'\n",
    "pinecone_key = yaml_data['PINECONE']['API_KEY']\n",
    "pinecone_env = yaml_data['PINECONE']['ENV']\n",
    "pinecone.init(\n",
    "    api_key=pinecone_key,  # find at app.pinecone.io\n",
    "    environment=pinecone_env  # next to api key in console\n",
    ")\n",
    "docsearch = Pinecone.from_documents(documents, oai, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss = FAISS.from_documents(docs_cta,oai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e407f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.as_retriever(search_kwargs={\"k\": 10}).get_relevant_documents(\"What is increase in CTA's cash flow from FY 2023 to FY 2021?\")[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d608a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.save_local('entiredocument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_db = FAISS.load_local('entiredocument',oai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is increase in CTA's payables from FY 2022 to FY 2021?\"\n",
    "faiss.as_retriever(search_kwargs={\"k\": 5}).get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.as_retriever(search_kwargs={\"k\": 1}).get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qachain(vectorstore,query):\n",
    "    ###Check if there are multiple files being fetched. Else stay to 5 documents\n",
    "    filter_dict = {'$and':[{'ticker':self.ticker},{'metadata':{'$ne':'Sentiment News'}}]}\n",
    "    documents = vectorstore.as_retriever(search_kwargs={\"k\": 5,filter = filter_dict}).get_relevant_documents(query)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    k_count = min(len(set([doc.metadata['file_path'] for doc in documents])),3)*5\n",
    "    if k_count != 5:\n",
    "        documents = vectorstore.as_retriever(search_kwargs={\"k\": k_count}).get_relevant_documents(query)    \n",
    "    #page_content = vectorstore.as_retriever(search_kwargs={\"k\": 10}).get_relevant_documents(query)\n",
    "    page_content = '\\n\\n'.join([doc.page_content for doc in documents])\n",
    "    meta_data = documents[0].metadata\n",
    "   # file_path = \n",
    "    context_precursor =  '''The below contains information about {} and you are a financial analyst'''.format(meta_data['ticker'])\n",
    "   # import pdb;pdb.set_trace()\n",
    "    prompt_template = \"\"\"Use the following information to answer the question at the end in a coherent summary. \n",
    "{context_precursor}\n",
    "{page_content}\n",
    "Question: {question}\n",
    "Think step by step. If there is not sufficient information provided, just say you don't know.\n",
    "\"\"\"\n",
    "    prompt = prompt_template.format(context_precursor = context_precursor,page_content = page_content,question = query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e40496",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type = qachain(faiss,\"What is increase in CTA's payables from FY 2022 to FY 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13488822",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Cohere(**cohere_params)\n",
    "llm = OpenAI(**open_ai_params)\n",
    "#llm = AI21(**ai21_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c09acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(\"financial-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ac8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_existing_index('financial-analysis', oai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch.similarity_search('How are you doing?',filter = {\"metadata\":'Key Financial Ratios'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docsearch.as_retriever(search_kwargs={\"k\": 100,\"filter\":filter_dict}).get_relevant_documents('Selling and administrative expenses as a percent of revenue for the three months?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"apis.yaml\", \"r\") as file:\n",
    "    yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "openbb.keys.finnhub(key=data_dict[\"OPENBB\"][\"FINNHUB_KEY\"], persist=True)\n",
    "import os\n",
    "list1 = []\n",
    "from openbb_terminal.sdk import openbb\n",
    "from datetime import datetime as datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "start_date = (datetime.today() - relativedelta(months=2)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ")\n",
    "import pandas as pd\n",
    "for ticker in os.listdir('../ticker'):\n",
    "    try:\n",
    "        if ticker == \"BRK-B\":\n",
    "            df = openbb.stocks.ba.cnews(\n",
    "                \"BRK.A\", start_date=start_date, end_date=end_date\n",
    "            )\n",
    "            df.insert(0,'ticker',ticker)\n",
    "        else:\n",
    "            df = openbb.stocks.ba.cnews(\n",
    "                ticker, start_date=start_date, end_date=end_date\n",
    "            )\n",
    "        time.sleep(1)\n",
    "        df = pd.DataFrame(df)[[\"related\", \"datetime\", \"headline\", \"summary\"]]\n",
    "        df[\"datetime\"] = df[\"datetime\"].apply(\n",
    "            lambda x: datetime.fromtimestamp(x)\n",
    "        )\n",
    "        choices = list(\n",
    "            stock_summary[stock_summary[\"Ticker\"] == ticker].values[0]\n",
    "        )\n",
    "        result = pd.DataFrame(\n",
    "            [\n",
    "                process.extract(headline, choices, limit=2)\n",
    "                for headline in df[\"headline\"]\n",
    "            ]\n",
    "        )\n",
    "        result.columns = choices\n",
    "        result[choices[0]] = [x[1] for x in result[choices[0]]]\n",
    "        result[choices[1]] = [x[1] for x in result[choices[1]]]\n",
    "        result[\"headline\"] = df[\"headline\"]\n",
    "        result[\"final_score\"] = [\n",
    "            max(x, y) for x, y in zip(result[choices[0]], result[choices[1]])\n",
    "        ]\n",
    "        result = result[result[\"final_score\"] > 50][[\"headline\", \"final_score\"]]\n",
    "        result[\"datetime\"] = df[\"datetime\"]\n",
    "        list1.append(result)\n",
    "    except:\n",
    "        pass\n",
    "#         list1.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d63857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Forecasting Toolkit is disabled. To use the Forecasting features please install the toolkit following the instructions here: https://docs.openbb.co/sdk/quickstart/installation/</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mForecasting Toolkit is disabled. To use the Forecasting features please install the toolkit following the instructions here: https://docs.openbb.co/sdk/quickstart/installation/\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Portfolio Optimization Toolkit is disabled. To use the Optimization features please install the toolkit following the instructions here: https://docs.openbb.co/sdk/quickstart/installation/</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mPortfolio Optimization Toolkit is disabled. To use the Optimization features please install the toolkit following the instructions here: https://docs.openbb.co/sdk/quickstart/installation/\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "from openbb_terminal.sdk import openbb\n",
    "from datetime import datetime as datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "start_date = (datetime.today() - relativedelta(months=2)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ")\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "with open(\"apis.yaml\", \"r\") as file:\n",
    "    yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    data_dict = dict(yaml_data)\n",
    "openbb.keys.finnhub(key=data_dict[\"OPENBB\"][\"FINNHUB_KEY\"], persist=True)\n",
    "import os\n",
    "import time\n",
    "stock_summary = pd.read_json(\"https://www.sec.gov/files/company_tickers.json\").T\n",
    "stock_summary = stock_summary[[\"title\", \"ticker\"]]\n",
    "stock_summary.columns = [\"Company\", \"Ticker\"]\n",
    "list1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fe685",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in os.listdir('../ticker'):\n",
    "    try:\n",
    "        if ticker == \"BRK-B\":\n",
    "            df = openbb.stocks.ba.cnews(\n",
    "                \"BRK.A\", start_date=start_date, end_date=end_date\n",
    "            )\n",
    "            df.insert(0,'ticker',ticker)\n",
    "        else:\n",
    "            df = openbb.stocks.ba.cnews(\n",
    "                ticker, start_date=start_date, end_date=end_date\n",
    "            )\n",
    "        time.sleep(1)\n",
    "        \n",
    "        df = pd.DataFrame(df)[[\"related\", \"datetime\", \"headline\", \"summary\"]]\n",
    "        df[\"datetime\"] = df[\"datetime\"].apply(\n",
    "            lambda x: datetime.fromtimestamp(x)\n",
    "        )\n",
    "        choices = list(\n",
    "            stock_summary[stock_summary[\"Ticker\"] == ticker].values[0]\n",
    "        )\n",
    "        result = pd.DataFrame(\n",
    "            [\n",
    "                process.extract(headline, choices, limit=2)\n",
    "                for headline in df[\"headline\"]\n",
    "            ]\n",
    "        )\n",
    "        result.columns = choices\n",
    "        result[choices[0]] = [x[1] for x in result[choices[0]]]\n",
    "        result[choices[1]] = [x[1] for x in result[choices[1]]]\n",
    "        result[\"headline\"] = df[\"headline\"]\n",
    "        result[\"final_score\"] = [\n",
    "            max(x, y) for x, y in zip(result[choices[0]], result[choices[1]])\n",
    "        ]\n",
    "        #import pdb;pdb.set_trace()\n",
    "        result = result[result[\"final_score\"] > 50][[\"headline\", \"final_score\"]]\n",
    "        result[\"datetime\"] = df[\"datetime\"]\n",
    "        result.insert(0,'ticker',ticker)\n",
    "        list1.append(result)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab884cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CohereEmbeddings,OpenAIEmbeddings\n",
    "oai = OpenAIEmbeddings(openai_api_key = yaml_data[\"LLMS\"]['OPENAI_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40899f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv('sample_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c830ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = ['This is extremly positive news for my stock and it will rise highly today.',\\\n",
    "                   'This will have a positive impact on my stock','There is no impact on the share price',\\\n",
    "                   'This will have a negative impact on my stock',\\\n",
    "                   'This is terrible news and the share price will drop significantly']\n",
    "        ## Create a faiss vector database\n",
    "        #import pdb;pdb.set_trace()\n",
    "faiss_classifications = FAISS.from_texts(classifications,oai)\n",
    "similarity_scores = []\n",
    "x = pd.read_csv('sample_input.csv')\n",
    "x.drop('Unnamed: 0',axis = 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8522f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be54da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 32352/32352 [3:11:13<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiments = []\n",
    "for headline in tqdm(list(x['headline'])):\n",
    "    sentiments.append(faiss_classifications.similarity_search_with_score(headline,k=1)[0][0].page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af404938",
   "metadata": {},
   "outputs": [],
   "source": [
    "x['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c1d9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv('sample_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8332bd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70198.02299210428"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1231940608**0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1231940608**0.5)*2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
